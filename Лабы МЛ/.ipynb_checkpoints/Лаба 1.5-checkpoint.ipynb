{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import metrics\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, HashingVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import learning_curve\n",
    "from scipy.stats import sem\n",
    "import time\n",
    "import IPython\n",
    "import sklearn as sk\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разобью на тестовые и применю мешок слов, шоб было, но использовать буду тфидф)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "news = fetch_20newsgroups(subset='all')\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(news.data)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(news.data, news.target, test_size=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Взял стоп- слова вот отсюда, не стал инсталлить, просто скачал текстовик, который приложу к лабе.\n",
    "https://github.com/stopwords-iso/stopwords-en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stop_words():\n",
    "    result = set()\n",
    "    for line in open('stopw_en.txt', 'r').readlines():\n",
    "        result.add(line.strip())\n",
    "    return result\n",
    "stop_words = get_stop_words()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция, реализующаяя кросс-валидацию Можно было и на гриде, но ниже увидите мои потуги- там что то сломалось из за пайпа."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_cross_validation(clf, X, y, K):\n",
    "    cv = KFold(K, shuffle=True, random_state=0).get_n_splits(len(y))\n",
    "    scores = cross_val_score(clf, X, y, cv=cv)\n",
    "    print(scores)\n",
    "    print((\"Mean score: {0:.3f} (+/-{1:.3f})\").format(\n",
    "        np.mean(scores), sem(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_001 = Pipeline([\n",
    "    ('vect', TfidfVectorizer(\n",
    "                stop_words=stop_words,\n",
    "                token_pattern=r\"\\b[a-z0-9_\\-\\.]+[a-z][a-z0-9_\\-\\.]+\\b\",    \n",
    "    )),\n",
    "    ('clf', MultinomialNB(0.01)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_01 = Pipeline([\n",
    "    ('vect', TfidfVectorizer(\n",
    "                stop_words=stop_words,\n",
    "                token_pattern=r\"\\b[a-z0-9_\\-\\.]+[a-z][a-z0-9_\\-\\.]+\\b\",    \n",
    "    )),\n",
    "    ('clf', MultinomialNB(0.1)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_02 = Pipeline([\n",
    "    ('vect', TfidfVectorizer(\n",
    "                stop_words=stop_words,\n",
    "                token_pattern=r\"\\b[a-z0-9_\\-\\.]+[a-z][a-z0-9_\\-\\.]+\\b\",    \n",
    "    )),\n",
    "    ('clf', MultinomialNB(alpha=0.2)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_03 = Pipeline([\n",
    "    ('vect', TfidfVectorizer(\n",
    "                stop_words=stop_words,\n",
    "                token_pattern=r\"\\b[a-z0-9_\\-\\.]+[a-z][a-z0-9_\\-\\.]+\\b\",    \n",
    "    )),\n",
    "    ('clf', MultinomialNB(alpha=0.3)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_04 = Pipeline([\n",
    "    ('vect', TfidfVectorizer(\n",
    "                stop_words=stop_words,\n",
    "                token_pattern=r\"\\b[a-z0-9_\\-\\.]+[a-z][a-z0-9_\\-\\.]+\\b\",    \n",
    "    )),\n",
    "    ('clf', MultinomialNB(alpha=0.4)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_05 = Pipeline([\n",
    "    ('vect', TfidfVectorizer(\n",
    "                stop_words=stop_words,\n",
    "                token_pattern=r\"\\b[a-z0-9_\\-\\.]+[a-z][a-z0-9_\\-\\.]+\\b\",    \n",
    "    )),\n",
    "    ('clf', MultinomialNB(alpha=0.5)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.92599469 0.9265057  0.92624038 0.91642345 0.92066861]\n",
      "Mean score: 0.923 (+/-0.002)\n",
      "[0.9137931  0.92305651 0.91881136 0.90925975 0.9097904 ]\n",
      "Mean score: 0.915 (+/-0.003)\n",
      "[0.91034483 0.91642345 0.91350491 0.90342266 0.90130008]\n",
      "Mean score: 0.909 (+/-0.003)\n",
      "[0.90318302 0.91297426 0.91164765 0.90050411 0.89599363]\n",
      "Mean score: 0.905 (+/-0.003)\n",
      "[0.89893899 0.90846378 0.90687185 0.89493234 0.89280976]\n",
      "Mean score: 0.900 (+/-0.003)\n",
      "[0.89602122 0.90581056 0.90421863 0.89387105 0.89227912]\n",
      "Mean score: 0.898 (+/-0.003)\n"
     ]
    }
   ],
   "source": [
    "for i in [clf_001, clf_01, clf_02, clf_03, clf_04, clf_05]:\n",
    "    evaluate_cross_validation(i, news.data, news.target, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(clf, X_train, X_test, y_train, y_test):\n",
    "    clf.fit(X_train, y_train)\n",
    "    print(\"Accuracy on training set: \", clf.score(X_train, y_train))\n",
    "    print(\"Accuracy on testing set: \", clf.score(X_test, y_test))\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(\"Classification Report: \", metrics.classification_report(y_test, y_pred))\n",
    "    print(\"Confusion Matrix: \", metrics.confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set:  0.9966746851563606\n",
      "Accuracy on testing set:  0.9206281833616299\n",
      "Classification Report:                precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.95      0.93       198\n",
      "           1       0.81      0.88      0.84       228\n",
      "           2       0.88      0.82      0.85       265\n",
      "           3       0.76      0.85      0.80       246\n",
      "           4       0.89      0.92      0.91       221\n",
      "           5       0.92      0.89      0.90       267\n",
      "           6       0.92      0.82      0.87       220\n",
      "           7       0.96      0.95      0.95       260\n",
      "           8       0.97      0.98      0.97       257\n",
      "           9       0.96      0.96      0.96       244\n",
      "          10       0.96      0.97      0.97       256\n",
      "          11       0.95      0.96      0.95       234\n",
      "          12       0.94      0.89      0.91       253\n",
      "          13       0.95      0.95      0.95       244\n",
      "          14       0.95      0.97      0.96       254\n",
      "          15       0.95      0.95      0.95       257\n",
      "          16       0.91      0.95      0.93       233\n",
      "          17       0.98      1.00      0.99       254\n",
      "          18       0.93      0.89      0.91       169\n",
      "          19       0.92      0.80      0.86       152\n",
      "\n",
      "    accuracy                           0.92      4712\n",
      "   macro avg       0.92      0.92      0.92      4712\n",
      "weighted avg       0.92      0.92      0.92      4712\n",
      "\n",
      "Confusion Matrix:  [[188   0   0   0   0   0   0   0   0   0   0   0   0   0   0   7   0   0\n",
      "    0   3]\n",
      " [  1 201   5   7   2   8   0   0   0   1   0   0   0   0   2   0   1   0\n",
      "    0   0]\n",
      " [  0   9 216  26   1   7   2   0   0   0   1   1   1   0   0   1   0   0\n",
      "    0   0]\n",
      " [  0   4  10 210  11   0   3   1   0   0   0   1   4   0   0   0   0   0\n",
      "    0   2]\n",
      " [  0   3   2   5 204   2   3   0   0   0   0   0   2   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0  19   3   2   2 237   1   0   0   1   0   0   0   2   0   0   0   0\n",
      "    0   0]\n",
      " [  0   2   1  15   2   1 181   2   2   2   2   2   2   2   3   0   1   0\n",
      "    0   0]\n",
      " [  0   0   1   0   1   0   1 246   6   0   0   1   3   0   0   0   1   0\n",
      "    0   0]\n",
      " [  0   0   0   0   0   0   0   2 253   1   0   0   0   0   0   0   1   0\n",
      "    0   0]\n",
      " [  0   0   0   0   0   0   0   1   1 234   5   0   0   2   0   0   1   0\n",
      "    0   0]\n",
      " [  0   1   1   0   1   0   1   0   0   2 249   0   0   1   0   0   0   0\n",
      "    0   0]\n",
      " [  0   2   1   0   0   3   1   1   0   0   0 224   0   0   0   0   1   1\n",
      "    0   0]\n",
      " [  0   3   4   6   3   0   2   2   0   0   1   2 226   1   3   0   0   0\n",
      "    0   0]\n",
      " [  1   0   0   4   0   0   0   0   0   0   0   1   2 233   1   0   2   0\n",
      "    0   0]\n",
      " [  1   2   0   0   0   0   0   0   0   0   0   1   1   0 247   0   0   0\n",
      "    1   1]\n",
      " [  2   1   1   1   1   0   0   0   0   0   1   1   0   1   2 243   0   0\n",
      "    0   3]\n",
      " [  0   0   0   0   0   1   1   1   0   1   0   1   0   0   0   0 221   0\n",
      "    6   1]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 253\n",
      "    1   0]\n",
      " [  0   0   0   0   0   0   0   0   0   1   0   1   0   1   1   0  11   3\n",
      "  150   1]\n",
      " [ 12   1   1   0   0   0   0   0   0   0   0   0   0   1   1   6   4   1\n",
      "    3 122]]\n"
     ]
    }
   ],
   "source": [
    "train_and_evaluate(clf_001, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Немного переобучено, на accuracy отличное"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Код ниже не запускайте, это моя неудачная попытака пихнуть все в грид, он постоянно и беспричинно ругается на то, что ему нужно передать set, а я передаю ему set, хотя надо передавать set =)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfv = TfidfVectorizer()\n",
    "multnb = MultinomialNB()\n",
    "pipe = Pipeline(steps=[('tfv', tfv), ('multnb', multnb)])\n",
    "params = {\n",
    "    'tfv_stop_words': stop_words,\n",
    "    'tfv__token_pattern': r\"\\b[a-z0-9_\\-\\.]+[a-z][a-z0-9_\\-\\.]+\\b\",\n",
    "    'multnb__alpha': [0.01, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "}\n",
    "search = GridSearchCV(pipe, params, n_jobs=-1, cv=5)\n",
    "search.fit(news.data, news.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Код выше не запускайте, это моя неудачная попытака пихнуть все в грид, он постоянно и беспричинно ругается на то, что ему нужно передать set, а я передаю ему set, хотя надо передавать set =)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    LogisticRegression(max_iter=100000, n_jobs = -1),\n",
    "    svm.SVC(kernel='linear'),\n",
    "    svm.SVC(kernel='rbf'),\n",
    "    KNeighborsClassifier(n_jobs=-1),\n",
    "    MLPClassifier(max_iter=100000, verbose=True)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set:  0.9757322767793972\n",
      "Accuracy on testing set:  0.9068336162988115\n",
      "Classification Report:                precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.90      0.91       198\n",
      "           1       0.78      0.89      0.83       228\n",
      "           2       0.84      0.84      0.84       265\n",
      "           3       0.78      0.77      0.77       246\n",
      "           4       0.87      0.90      0.88       221\n",
      "           5       0.92      0.88      0.90       267\n",
      "           6       0.87      0.89      0.88       220\n",
      "           7       0.94      0.92      0.93       260\n",
      "           8       0.96      0.98      0.97       257\n",
      "           9       0.96      0.98      0.97       244\n",
      "          10       0.98      0.97      0.97       256\n",
      "          11       0.99      0.94      0.97       234\n",
      "          12       0.83      0.90      0.86       253\n",
      "          13       0.95      0.93      0.94       244\n",
      "          14       0.98      0.97      0.97       254\n",
      "          15       0.89      0.93      0.91       257\n",
      "          16       0.92      0.93      0.92       233\n",
      "          17       0.98      0.98      0.98       254\n",
      "          18       0.93      0.84      0.88       169\n",
      "          19       0.89      0.69      0.78       152\n",
      "\n",
      "    accuracy                           0.91      4712\n",
      "   macro avg       0.91      0.90      0.90      4712\n",
      "weighted avg       0.91      0.91      0.91      4712\n",
      "\n",
      "Confusion Matrix:  [[179   0   0   0   0   0   0   0   0   1   0   0   0   0   0  10   0   0\n",
      "    0   8]\n",
      " [  1 202   8   3   5   6   1   0   0   0   0   0   2   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0   7 223  23   4   6   0   0   0   0   0   0   2   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0   7  19 190   8   2   6   0   0   0   1   0  13   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0   6   2   8 198   1   2   0   0   0   0   0   4   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0  15   7   3   3 234   1   0   0   0   0   0   2   2   0   0   0   0\n",
      "    0   0]\n",
      " [  0   3   0   7   2   1 196   3   0   1   0   1   2   3   1   0   0   0\n",
      "    0   0]\n",
      " [  0   0   1   1   0   0   2 239  10   0   0   0   6   1   0   0   0   0\n",
      "    0   0]\n",
      " [  0   0   1   1   0   0   3   0 251   0   0   0   1   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0   1   0   0   0   0   0   0   0 240   3   0   0   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0   1   1   0   0   0   1   0   0   3 249   0   1   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0   3   1   1   0   2   2   0   0   1   0 221   1   0   0   0   1   0\n",
      "    1   0]\n",
      " [  0   3   2   6   4   0   3   5   0   0   2   0 227   0   1   0   0   0\n",
      "    0   0]\n",
      " [  1   1   0   1   3   0   4   2   0   0   0   0   4 226   2   0   0   0\n",
      "    0   0]\n",
      " [  0   2   0   1   0   0   1   0   0   0   0   0   0   1 247   0   1   0\n",
      "    1   0]\n",
      " [  3   3   1   0   0   1   2   0   0   1   0   0   2   0   1 238   0   1\n",
      "    0   4]\n",
      " [  0   2   0   0   0   1   0   2   0   2   0   0   3   2   0   0 217   0\n",
      "    4   0]\n",
      " [  0   1   0   0   0   0   0   0   0   1   0   0   0   1   0   1   0 249\n",
      "    1   0]\n",
      " [  1   0   0   0   0   1   1   3   0   1   0   1   0   0   0   0  15   3\n",
      "  142   1]\n",
      " [ 12   1   0   0   0   0   0   1   0   0   0   0   2   3   1  19   3   2\n",
      "    3 105]]\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100000,\n",
      "                   multi_class='auto', n_jobs=-1, penalty='l2',\n",
      "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
      "                   warm_start=False) \n",
      "Время обучения  66.39265871047974 \n",
      "Score\n",
      "Accuracy on training set:  0.9939861327295882\n",
      "Accuracy on testing set:  0.9248726655348047\n",
      "Classification Report:                precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.92      0.93       198\n",
      "           1       0.80      0.89      0.84       228\n",
      "           2       0.87      0.87      0.87       265\n",
      "           3       0.77      0.83      0.80       246\n",
      "           4       0.90      0.90      0.90       221\n",
      "           5       0.92      0.89      0.90       267\n",
      "           6       0.93      0.91      0.92       220\n",
      "           7       0.97      0.93      0.95       260\n",
      "           8       0.97      0.97      0.97       257\n",
      "           9       0.95      0.99      0.97       244\n",
      "          10       0.99      0.97      0.98       256\n",
      "          11       0.99      0.95      0.97       234\n",
      "          12       0.85      0.93      0.89       253\n",
      "          13       0.95      0.93      0.94       244\n",
      "          14       1.00      0.97      0.98       254\n",
      "          15       0.95      0.93      0.94       257\n",
      "          16       0.95      0.94      0.95       233\n",
      "          17       0.99      0.98      0.99       254\n",
      "          18       0.96      0.91      0.93       169\n",
      "          19       0.91      0.82      0.86       152\n",
      "\n",
      "    accuracy                           0.92      4712\n",
      "   macro avg       0.93      0.92      0.92      4712\n",
      "weighted avg       0.93      0.92      0.93      4712\n",
      "\n",
      "Confusion Matrix:  [[183   0   0   0   0   0   0   0   0   1   0   0   0   0   0   6   0   0\n",
      "    0   8]\n",
      " [  1 203   7   6   4   5   0   0   0   1   0   0   1   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0   4 231  21   3   5   0   0   0   0   0   0   1   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0   6  14 203   8   1   4   0   0   0   0   0  10   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0   4   1   9 200   0   3   0   0   0   0   0   4   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0  12   8   4   2 237   1   0   0   0   0   0   2   1   0   0   0   0\n",
      "    0   0]\n",
      " [  0   2   0   7   1   1 201   1   0   2   0   0   2   3   0   0   0   0\n",
      "    0   0]\n",
      " [  0   0   0   1   0   0   1 243   8   0   0   0   6   1   0   0   0   0\n",
      "    0   0]\n",
      " [  0   1   1   1   0   0   1   1 249   1   0   0   2   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0   0   0   0   0   0   0   0   0 242   2   0   0   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0   2   1   0   0   0   0   0   0   3 248   0   1   1   0   0   0   0\n",
      "    0   0]\n",
      " [  0   2   1   1   0   3   1   0   0   0   0 223   1   0   0   0   1   0\n",
      "    1   0]\n",
      " [  0   3   1   8   2   0   1   2   0   0   1   0 235   0   0   0   0   0\n",
      "    0   0]\n",
      " [  1   4   1   1   1   0   1   1   0   1   0   0   5 228   0   0   0   0\n",
      "    0   0]\n",
      " [  0   2   0   1   0   1   1   0   0   0   0   0   1   1 246   0   0   0\n",
      "    1   0]\n",
      " [  2   3   1   0   0   1   1   0   0   2   0   0   2   1   0 240   0   0\n",
      "    0   4]\n",
      " [  0   3   0   0   0   1   0   1   0   2   0   1   2   1   0   0 219   0\n",
      "    3   0]\n",
      " [  0   0   0   1   0   0   0   0   0   1   0   0   0   1   0   1   0 250\n",
      "    0   0]\n",
      " [  1   0   0   0   0   1   1   2   0   0   0   1   0   0   0   0   7   2\n",
      "  153   1]\n",
      " [  9   2   0   0   0   1   0   0   0   0   0   0   1   3   1   6   3   1\n",
      "    1 124]]\n",
      "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear',\n",
      "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "    tol=0.001, verbose=False) \n",
      "Время обучения  424.8799331188202 \n",
      "Score\n",
      "Accuracy on training set:  0.9974529503325315\n",
      "Accuracy on testing set:  0.9129881154499151\n",
      "Classification Report:                precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.91      0.92       198\n",
      "           1       0.73      0.91      0.81       228\n",
      "           2       0.86      0.84      0.85       265\n",
      "           3       0.78      0.80      0.79       246\n",
      "           4       0.93      0.90      0.92       221\n",
      "           5       0.93      0.87      0.90       267\n",
      "           6       0.90      0.91      0.91       220\n",
      "           7       0.96      0.92      0.94       260\n",
      "           8       0.97      0.96      0.96       257\n",
      "           9       0.96      0.99      0.97       244\n",
      "          10       0.99      0.96      0.98       256\n",
      "          11       0.99      0.94      0.96       234\n",
      "          12       0.80      0.93      0.86       253\n",
      "          13       0.93      0.93      0.93       244\n",
      "          14       0.99      0.97      0.98       254\n",
      "          15       0.92      0.93      0.93       257\n",
      "          16       0.92      0.93      0.93       233\n",
      "          17       0.99      0.96      0.98       254\n",
      "          18       0.94      0.85      0.89       169\n",
      "          19       0.92      0.78      0.84       152\n",
      "\n",
      "    accuracy                           0.91      4712\n",
      "   macro avg       0.92      0.91      0.91      4712\n",
      "weighted avg       0.92      0.91      0.91      4712\n",
      "\n",
      "Confusion Matrix:  [[181   0   0   0   0   0   0   0   0   0   0   0   1   0   0   8   0   0\n",
      "    0   8]\n",
      " [  1 208   6   4   3   4   0   0   0   0   0   0   2   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0  12 222  22   2   5   0   0   0   0   0   0   2   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0   8  17 197   6   2   4   0   0   0   0   0  12   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0   5   1   7 200   0   4   0   0   0   0   0   4   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0  18   8   3   1 232   1   0   0   0   0   0   2   2   0   0   0   0\n",
      "    0   0]\n",
      " [  0   3   0   7   1   1 201   1   0   2   0   0   1   3   0   0   0   0\n",
      "    0   0]\n",
      " [  0   1   1   0   0   0   1 238   8   0   0   0  10   1   0   0   0   0\n",
      "    0   0]\n",
      " [  0   1   1   1   0   0   3   1 247   1   0   0   1   0   0   0   1   0\n",
      "    0   0]\n",
      " [  0   0   0   0   0   0   0   0   0 241   2   0   1   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0   1   1   0   0   0   1   0   0   3 246   0   3   1   0   0   0   0\n",
      "    0   0]\n",
      " [  0   5   1   1   0   2   1   0   0   0   0 220   2   0   0   0   1   0\n",
      "    1   0]\n",
      " [  0   2   1   8   0   0   2   2   0   1   0   0 235   1   1   0   0   0\n",
      "    0   0]\n",
      " [  1   4   0   1   1   0   4   1   0   0   0   0   6 226   0   0   0   0\n",
      "    0   0]\n",
      " [  0   3   0   1   0   0   0   0   0   0   0   0   3   1 246   0   0   0\n",
      "    0   0]\n",
      " [  2   7   0   0   0   0   1   0   0   1   0   0   4   0   0 240   0   0\n",
      "    0   2]\n",
      " [  0   3   0   0   0   1   0   1   0   2   0   1   2   2   0   0 216   0\n",
      "    5   0]\n",
      " [  0   2   0   1   0   1   0   0   0   1   0   0   0   2   0   1   0 245\n",
      "    1   0]\n",
      " [  0   1   0   0   0   1   0   5   0   0   0   1   2   0   0   1  13   2\n",
      "  143   0]\n",
      " [  9   2   0   0   0   0   0   0   0   0   0   0   2   4   1  10   3   1\n",
      "    2 118]]\n",
      "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
      "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "    tol=0.001, verbose=False) \n",
      "Время обучения  538.8465993404388 \n",
      "Score\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set:  0.8999575491722088\n",
      "Accuracy on testing set:  0.8408319185059423\n",
      "Classification Report:                precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.90      0.84       198\n",
      "           1       0.72      0.82      0.77       228\n",
      "           2       0.72      0.84      0.77       265\n",
      "           3       0.65      0.71      0.68       246\n",
      "           4       0.70      0.73      0.72       221\n",
      "           5       0.84      0.77      0.80       267\n",
      "           6       0.75      0.57      0.65       220\n",
      "           7       0.83      0.82      0.82       260\n",
      "           8       0.92      0.91      0.92       257\n",
      "           9       0.92      0.90      0.91       244\n",
      "          10       0.91      0.96      0.94       256\n",
      "          11       0.89      0.94      0.91       234\n",
      "          12       0.83      0.77      0.80       253\n",
      "          13       0.90      0.81      0.86       244\n",
      "          14       0.93      0.95      0.94       254\n",
      "          15       0.90      0.86      0.88       257\n",
      "          16       0.93      0.88      0.91       233\n",
      "          17       0.96      0.99      0.98       254\n",
      "          18       0.93      0.85      0.89       169\n",
      "          19       0.81      0.77      0.79       152\n",
      "\n",
      "    accuracy                           0.84      4712\n",
      "   macro avg       0.84      0.84      0.84      4712\n",
      "weighted avg       0.84      0.84      0.84      4712\n",
      "\n",
      "Confusion Matrix:  [[178   1   0   0   0   0   0   0   0   0   0   0   0   1   0  10   0   0\n",
      "    0   8]\n",
      " [  1 186   9   4   3  11   2   2   0   1   0   3   1   1   1   1   0   1\n",
      "    1   0]\n",
      " [  1   5 222  20   3   4   3   0   1   0   1   0   3   1   0   1   0   0\n",
      "    0   0]\n",
      " [  0   8  21 175  16   2   6   2   0   3   0   2   8   1   0   0   0   0\n",
      "    0   2]\n",
      " [  1   5   6  29 162   0   7   0   1   1   1   0   4   0   1   1   0   1\n",
      "    0   1]\n",
      " [  2  18  18   4   4 205   4   0   3   2   0   1   0   3   1   0   0   1\n",
      "    0   1]\n",
      " [  2   5  13  16  14   3 126   9   1   1   8   3   6   5   1   2   0   1\n",
      "    3   1]\n",
      " [  0   5   3   2   8   1   3 213   9   2   1   1   8   1   1   0   2   0\n",
      "    0   0]\n",
      " [  1   0   1   3   1   1   0  10 234   1   0   0   0   1   0   0   4   0\n",
      "    0   0]\n",
      " [  2   2   1   0   1   3   2   3   0 220   4   0   2   1   0   1   0   0\n",
      "    1   1]\n",
      " [  0   1   2   0   0   0   1   1   0   3 246   0   1   0   1   0   0   0\n",
      "    0   0]\n",
      " [  0   0   2   1   0   5   4   1   0   1   0 219   0   0   1   0   0   0\n",
      "    0   0]\n",
      " [  3   6   5   9  12   2   7   5   0   0   2   1 196   0   3   1   1   0\n",
      "    0   0]\n",
      " [  6   7   3   4   5   2   3   3   0   1   0   1   6 198   2   0   2   0\n",
      "    1   0]\n",
      " [  1   2   1   0   0   1   0   0   0   0   2   3   1   0 242   0   0   0\n",
      "    1   0]\n",
      " [ 10   2   1   1   0   1   1   1   2   0   1   1   0   3   1 222   0   1\n",
      "    0   9]\n",
      " [  1   3   1   0   0   1   0   5   2   1   1   7   0   0   1   0 205   1\n",
      "    3   1]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   1   0   0   0   1   0 252\n",
      "    0   0]\n",
      " [  3   0   0   0   0   1   0   2   0   2   2   2   0   1   1   0   4   4\n",
      "  144   3]\n",
      " [ 16   2   0   0   2   0   0   0   0   1   0   0   1   2   2   6   2   0\n",
      "    1 117]]\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,\n",
      "                     weights='uniform') \n",
      "Время обучения  25.160542964935303 \n",
      "Score\n",
      "Iteration 1, loss = 2.64498319\n",
      "Iteration 2, loss = 1.38374113\n",
      "Iteration 3, loss = 0.53159292\n",
      "Iteration 4, loss = 0.23681121\n",
      "Iteration 5, loss = 0.12644924\n",
      "Iteration 6, loss = 0.07698514\n",
      "Iteration 7, loss = 0.05176653\n",
      "Iteration 8, loss = 0.03800860\n",
      "Iteration 9, loss = 0.02950515\n",
      "Iteration 10, loss = 0.02416084\n",
      "Iteration 11, loss = 0.02044217\n",
      "Iteration 12, loss = 0.01777775\n",
      "Iteration 13, loss = 0.01586905\n",
      "Iteration 14, loss = 0.01437958\n",
      "Iteration 15, loss = 0.01317431\n",
      "Iteration 16, loss = 0.01233697\n",
      "Iteration 17, loss = 0.01150246\n",
      "Iteration 18, loss = 0.01083109\n",
      "Iteration 19, loss = 0.01032648\n",
      "Iteration 20, loss = 0.00987279\n",
      "Iteration 21, loss = 0.00942449\n",
      "Iteration 22, loss = 0.00921391\n",
      "Iteration 23, loss = 0.00892991\n",
      "Iteration 24, loss = 0.00871218\n",
      "Iteration 25, loss = 0.00842132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:573: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set:  0.9997169944813924\n",
      "Accuracy on testing set:  0.9365449915110357\n",
      "Classification Report:                precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.93      0.93       198\n",
      "           1       0.85      0.90      0.87       228\n",
      "           2       0.87      0.86      0.87       265\n",
      "           3       0.80      0.84      0.82       246\n",
      "           4       0.92      0.94      0.93       221\n",
      "           5       0.91      0.93      0.92       267\n",
      "           6       0.93      0.90      0.91       220\n",
      "           7       0.96      0.95      0.96       260\n",
      "           8       0.97      0.99      0.98       257\n",
      "           9       0.97      0.98      0.98       244\n",
      "          10       0.97      0.98      0.97       256\n",
      "          11       0.99      0.97      0.98       234\n",
      "          12       0.92      0.91      0.92       253\n",
      "          13       0.97      0.96      0.96       244\n",
      "          14       0.98      0.98      0.98       254\n",
      "          15       0.96      0.93      0.94       257\n",
      "          16       0.96      0.98      0.97       233\n",
      "          17       0.98      0.99      0.99       254\n",
      "          18       0.99      0.91      0.94       169\n",
      "          19       0.90      0.87      0.88       152\n",
      "\n",
      "    accuracy                           0.94      4712\n",
      "   macro avg       0.94      0.93      0.94      4712\n",
      "weighted avg       0.94      0.94      0.94      4712\n",
      "\n",
      "Confusion Matrix:  [[185   0   0   0   0   0   0   0   0   0   0   0   0   0   1   5   0   0\n",
      "    0   7]\n",
      " [  0 205   7   5   2   9   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0   8 229  19   2   7   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0   7  14 207   7   1   3   0   0   0   0   0   7   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0   1   3   4 207   1   3   0   0   0   0   0   2   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0  10   5   1   1 247   1   0   0   1   0   0   0   1   0   0   0   0\n",
      "    0   0]\n",
      " [  0   3   0  10   1   1 198   3   0   0   1   0   1   2   0   0   0   0\n",
      "    0   0]\n",
      " [  0   0   1   0   2   0   0 248   6   0   0   0   3   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0   0   0   1   0   0   1   0 254   1   0   0   0   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0   0   0   0   0   0   0   0   0 240   4   0   0   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0   1   1   0   0   0   1   0   0   2 250   0   1   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0   1   0   1   0   3   0   0   0   0   0 228   0   0   0   0   0   0\n",
      "    1   0]\n",
      " [  0   2   1   8   2   0   3   2   0   0   2   1 230   1   1   0   0   0\n",
      "    0   0]\n",
      " [  1   0   0   1   0   0   1   1   1   2   0   0   2 234   1   0   0   0\n",
      "    0   0]\n",
      " [  1   2   0   0   0   0   1   0   0   0   0   0   1   0 248   0   1   0\n",
      "    0   0]\n",
      " [  1   1   1   1   1   0   1   0   0   0   1   0   2   1   1 239   0   1\n",
      "    0   6]\n",
      " [  0   1   0   0   0   1   0   0   0   1   0   0   0   0   0   0 228   0\n",
      "    1   1]\n",
      " [  0   0   0   0   0   0   0   0   0   1   0   0   0   1   0   1   0 251\n",
      "    0   0]\n",
      " [  1   0   0   0   0   0   0   3   0   0   0   1   0   0   0   1   7   2\n",
      "  153   1]\n",
      " [  9   0   1   0   0   1   0   0   0   0   0   0   0   2   1   4   1   1\n",
      "    0 132]]\n",
      "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
      "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "              hidden_layer_sizes=(100,), learning_rate='constant',\n",
      "              learning_rate_init=0.001, max_fun=15000, max_iter=100000,\n",
      "              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
      "              power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
      "              tol=0.0001, validation_fraction=0.1, verbose=True,\n",
      "              warm_start=False) \n",
      "Время обучения  1551.9445631504059 \n",
      "Score\n"
     ]
    }
   ],
   "source": [
    "for model in models:\n",
    "    clf = Pipeline([\n",
    "    ('vect', TfidfVectorizer(\n",
    "                stop_words=stop_words,\n",
    "                token_pattern=r\"\\b[a-z0-9_\\-\\.]+[a-z][a-z0-9_\\-\\.]+\\b\",    \n",
    "    )),\n",
    "    ('clf', model),\n",
    "    ])\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_and_evaluate(clf, X_train, X_test, y_train, y_test)\n",
    "\n",
    "    spend_time = time.time() - start_time\n",
    "    \n",
    "    print(\n",
    "        model,\n",
    "        \"\\nВремя обучения \",\n",
    "        spend_time,\n",
    "        \"\\nScore\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ужасно ужасно долго обучалось... Рок кривые будут строиться просто вечность, поэтому без них. Время по обучению каждой модели указано выше, всё подписано."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для быстрого результата лучше всего подойдет KNN- быстро обучается, быстро получаем результат. Если же времени много, то смело берем MLP- всего за 25 итераций мы получаем лучший accuracy на тестовой. Но это- ужасно дорого, либо руки у меня ужасно кривые. Что касается моего мнения- я всегда выбираю \"цена/качество\". В данном случае- Логистическая регрессия топ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Круто было бы сделать VotingClassifier из LogisticRegression и MultinomialNB, может бы что интересное и вышло. Но это уже совсем другая история. А вообще- нейронки в задачах классификации изображений и текстов вне конкурренции. Проблема лишь в их долгом обучении на слабых машинах. Да и на мощных- тоже долговато, но у меня, к сожалению, нет ничего, кроме моего core i5 8250U. На GPU пытаться смысла нет- у меня встроенная в проц."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
